{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "marketingDataDF = pd.read_csv(\"marketing_campaign.csv\")\n",
        "marketingDataDF.info()\n",
        "marketingDataDF.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "marketingDataDF = marketingDataDF.drop(marketingDataDF.columns[0], axis=1)\n",
        "nullValueNums = {}\n",
        "for data in marketingDataDF.columns:\n",
        "    nullValueNums[data] = marketingDataDF[data].isna().sum()\n",
        "\n",
        "totalRows = len(marketingDataDF)\n",
        "nullValueRatio = {col:round(num/totalRows,2) for col,num in nullValueNums.items()}\n",
        "\n",
        "nullSummaryDF = pd.DataFrame({\n",
        "    'Feature Name': list(nullValueNums.keys()),\n",
        "    'Number of NaNs': list(nullValueNums.values()),\n",
        "    'Ratio of NaNs': list(nullValueRatio.values())\n",
        "})\n",
        "\n",
        "display(nullSummaryDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# numericValues = marketingDataDF.select_dtypes(include=[np.number])\n",
        "# corrMatrix = oneHotEncoded.corr()\n",
        "# display(corrMatrix.style.background_gradient(cmap='coolwarm').format(\"{:.2f}\"))\n",
        "marketingDataDFEncoded = marketingDataDF.copy()\n",
        "label_encoder = LabelEncoder()\n",
        "for col in marketingDataDFEncoded.select_dtypes(include=[object]).columns:\n",
        "    marketingDataDFEncoded[col] = label_encoder.fit_transform(marketingDataDFEncoded[col])\n",
        "corrMatrix = marketingDataDFEncoded.corr()\n",
        "display(corrMatrix.style.background_gradient(cmap='coolwarm').format(\"{:.2f}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target = 'NumPurchases'\n",
        "sorted_features = corrMatrix[target].sort_values(ascending=False)\n",
        "top_four_features = sorted_features[1:5]\n",
        "for feature, corr in top_four_features.items():\n",
        "    print(feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for feature, corr in top_four_features.items():\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.xticks(rotation=90)\n",
        "    sb.countplot(data=marketingDataDFEncoded, x=feature)\n",
        "    plt.xticks(ticks=[], labels=[])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for feature, corr in top_four_features.items():\n",
        "    plt.figure()\n",
        "    sb.scatterplot(data=marketingDataDFEncoded, x=feature, y=target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8wJjybD0TaF"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0hxQLvI0TaF"
      },
      "source": [
        "Main form of simple linear regression function:\n",
        "$$f(x) = \\alpha x + \\beta$$\n",
        "\n",
        "here we want to find the bias ($\\alpha$) and slope($\\beta$) by minimizing the derivation of the Residual Sum of Squares (RSS) function:\n",
        "\n",
        "- step 1: Compute RSS of the training data  \n",
        "\n",
        "$$ RSS = \\Sigma (y_i - (\\hat{\\beta} + \\hat{\\alpha} * x_i) )^2 $$\n",
        "\n",
        "- step 2: Compute the derivatives of the RSS function in terms of $\\alpha$ and $\\beta$, and set them equal to 0 to find the desired parameters\n",
        "\n",
        "$$ \\frac{\\partial RSS}{\\partial \\beta} = \\Sigma (-f(x_i) + \\hat{\\beta} + \\hat{\\alpha} * x_i) = 0$$\n",
        "$$ \\to \\beta = \\hat{y} - \\hat{\\alpha} \\hat{x} \\to (1)$$\n",
        "\n",
        "\n",
        "$$ \\frac{\\partial RSS}{\\partial \\alpha} = \\Sigma (-2 x_i y_i + 2 \\hat{\\beta} x_i + 2\\hat{\\alpha} x_i ^ 2) = 0 \\to (2)$$\n",
        "\n",
        "$$ (1) , (2) \\to \\hat{\\alpha} = \\frac{\\Sigma{(x_i - \\hat{x})(y_i - \\hat{y})}}{\\Sigma{(x_i - \\hat{x})^2}}\n",
        "$$\n",
        "$$ \\hat{\\beta} = y - \\hat{a} x$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag7FoRHU0TaF"
      },
      "source": [
        "Based on the above formula, implement the function below to compute the parameters of a simple linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zh9olasP0TaG"
      },
      "outputs": [],
      "source": [
        "def simple_linear_regression(input_feature, output):\n",
        "    # ToDo\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Nwf9h-X0TaG"
      },
      "source": [
        "Now complete this `get_regression_predictions(...)` function to predict the value of given data based on the calculated intercept and slope"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6Pe7W6_0TaG"
      },
      "outputs": [],
      "source": [
        "def get_regression_predictions(input_feature, bias, slope):\n",
        "    # ToDo\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkAGHBgi0TaG"
      },
      "source": [
        "Now that we have a model and can make predictions, let's evaluate our model using Root Mean Square Error (RMSE). RMSE is the square root of the mean of the squared differences between the residuals, and the residuals is just a fancy word for the difference between the predicted output and the true output.\n",
        "\n",
        "Complete the following function to compute the RSME of a simple linear regression model given the input_feature, output, intercept and slope:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xPcfGRK0TaH"
      },
      "outputs": [],
      "source": [
        "def get_root_mean_square_error(predicted_values, outputs):\n",
        "    # ToDo\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJEeD-Cs0TaH"
      },
      "source": [
        "The RMSE has no bound, thus it becomes challenging to determine whether a particular RMSE value is considered good or bad without any reference point. Instead, we use R2 score. The R2 score is calculated by comparing the sum of the squared differences between the actual and predicted values of the dependent variable to the total sum of squared differences between the actual and mean values of the dependent variable. The R2 score is formulated as below:\n",
        "\n",
        "$$R^2 = 1 - \\frac{SSres}{SStot} = 1 - \\frac{\\sum_{i=1}^{n} (y_{i,true} - y_{i,pred})^2}{\\sum_{i=1}^{n} (y_{i,true} - \\bar{y}_{true})^2} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ctR7v2I0TaH"
      },
      "source": [
        "Complete the following function to calculate the R2 score of a given input_feature, output, bias, and slope:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rR4X-0du0TaI"
      },
      "outputs": [],
      "source": [
        "def get_r2_score(predicted_values, outputs):\n",
        "    # ToDo\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZWAjV5E0TaI"
      },
      "source": [
        "Now calculate the fitness of the model.\n",
        "Remember to provide explanation for the outputs in your code!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MS6MIiK0TaI"
      },
      "outputs": [],
      "source": [
        "designated_feature_list = [] # ToDo\n",
        "\n",
        "# ToDo\n",
        "#  measure the performance of the simple linear regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWRvVtWDnjkn"
      },
      "source": [
        "# Multiple Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm-NmSMlnjkn"
      },
      "source": [
        "Multiple regression is a statistical technique that aims to model the relationship between a dependent variable and two or more independent variables.\n",
        "\n",
        "Multiple regression with n independent variables is expressed as follows:\n",
        "\n",
        "$$f(x) = \\beta _{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} + \\beta_{3} x_{3} + \\beta_{4} x_{4} + ... + \\beta_{n} x_{n} + c $$\n",
        "\n",
        "To optimize the model for accurate predictions, multiple regression commonly employs iterative algorithms such as gradient descent.\n",
        "\n",
        "The main goal of the optimization process is to make our predictions as close as possible to the actual values.\n",
        "We measure the prediction error using a cost function, usually denoted as $J(\\beta)$.\n",
        "\n",
        "$$ J(\\beta)= \\frac {1}{2m} Î£_{i=0}^{m-1}(y_i - (\\hat \\beta _{0} + \\hat \\beta_{1} x_{1} + \\hat \\beta_{2} x_{2} + \\hat \\beta_{3} x_{3} + \\hat \\beta_{4} x_{4} + ... + \\hat \\beta_{n} x_{n}) )^2  $$\n",
        "\n",
        "Gradient descent iteratively adjusts the coefficients $(\\beta_i)$ to minimize the cost function. The update rule for each coefficient is:\n",
        "\n",
        "$$\\beta_{i} = \\beta _ {i} - \\alpha \\frac {âJ(\\beta)}{â\\beta_{i}}$$\n",
        "\n",
        "$$ \\frac {âJ(\\beta)}{â\\beta_{i}} = \\frac {1}{m}Î£_{j=0}^{m-1}(y_j - (\\hat \\beta _{0} + \\hat \\beta_{1} x_{j1} + \\hat \\beta_{2} x_{j2} + \\hat \\beta_{3} x_{j3} + \\hat \\beta_{4} x_{j4} + ... + \\hat \\beta_{n} x_{jn})) x_{ji} $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7_pbR7gLt5-"
      },
      "source": [
        "## Predicting output given regression weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIZ_-NnE9c2S"
      },
      "source": [
        "Based on the formula above and [np.dot()](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) method, complete this function to compute the predictions for an entire matrix of features given the matrix, bias, and the weights. Provide an explanation of np.dot method and the reasoning behind using this method in your code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsH4qtO-9jIL"
      },
      "outputs": [],
      "source": [
        "def predict_output(feature_matrix, weights, bias):\n",
        "    # assume feature_matrix is a numpy matrix containing the features as columns and weights is a corresponding numpy array\n",
        "    # create the predictions vector by using np.dot()\n",
        "    # ToDo\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWcDeSRmNRMl"
      },
      "source": [
        "## Computing the Derivative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHa9jl2rNTW4"
      },
      "source": [
        "As we saw, the cost function is the sum over the data points of the squared difference between an observed output and a predicted output.\n",
        "\n",
        "Since the derivative of a sum is the sum of the derivatives, we can compute the derivative for a single data point and then sum over data points. We can write the squared difference between the observed output and predicted output for a single point as follows:\n",
        "\n",
        "$$\n",
        "(output  - (const* w _{0} + [feature_1] * w_{1} + ...+ [feature_n] * w_{n}  ))^2\n",
        "$$\n",
        "\n",
        "With n feautures and a const , So the derivative will be :\n",
        "\n",
        "\n",
        "$$\n",
        "2 * (output  - (const* w _{0} + [feature_1] * w_{1} + ...+ [feature_n] * w_{n}  ))\n",
        "$$\n",
        "\n",
        "The term inside the paranethesis is just the error (difference between prediction and output). So we can re-write this as:\n",
        "\n",
        "$$2 * error*[feature_i] $$\n",
        "\n",
        "\n",
        "That is, the derivative for the weight for feature i is the sum (over data points) of 2 times the product of the error and the feature itself. In the case of the constant then this is just twice the sum of the errors!\n",
        "\n",
        "Recall that twice the sum of the product of two vectors is just twice the dot product of the two vectors. Therefore the derivative for the weight for feature_i is just two times the dot product between the values of feature_i and the current errors.\n",
        "\n",
        "\n",
        "With this in mind, complete the following derivative function which computes the derivative of the weight given the value of the feature (over all data points) and the errors (over all data points).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTbIOtbIRWXm"
      },
      "outputs": [],
      "source": [
        "def feature_derivative(errors, feature):\n",
        "    # ToDo\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_H8Ai3Wd_iW"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "Now we will write a function that performs a gradient descent. The basic premise is simple. Given a starting point we update the current weights by moving in the negative gradient direction. Recall that the gradient is the direction of increase and therefore the negative gradient is the direction of decrease and we're trying to minimize a cost function.\n",
        "\n",
        "\n",
        "The amount by which we move in the negative gradient direction is called the 'step size'. We stop when we are 'sufficiently close' to the optimum. We define this by requiring that the magnitude (length) of the gradient vector to be smaller than a fixed 'tolerance'.\n",
        "\n",
        "\n",
        "With this in mind, complete the following gradient descent function below using your derivative function above. For each step in the gradient descent we update the weight for each feature befofe computing our stopping criteria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFRqI1ZLgGfi"
      },
      "outputs": [],
      "source": [
        "def regression_gradient_descent(feature_matrix, outputs, initial_weights, bias, step_size, tolerance):\n",
        "    # ToDo\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOqgGKVfHgfy"
      },
      "outputs": [],
      "source": [
        "# Utility functions for multiple regression\n",
        "\n",
        "def normalize_features(chosen_features, data_frame):\n",
        "    for feature in chosen_features:\n",
        "        data_frame[feature] = (data_frame[feature] - data_frame[feature].mean()) / data_frame[feature].std()\n",
        "    return data_frame\n",
        "\n",
        "\n",
        "def n_feature_regression(chosen_feature_matrix, target_matrix, keywords):\n",
        "    initial_weights = keywords['initial_weights']\n",
        "    step_size = keywords['step_size']\n",
        "    tolerance = keywords['tolerance']\n",
        "    bias = keywords['bias']\n",
        "\n",
        "    weights, bias = regression_gradient_descent(chosen_feature_matrix, target_matrix, initial_weights, bias, step_size,\n",
        "                                                tolerance)\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "def get_weights_and_bias(chosen_features):\n",
        "    \"\"\"\n",
        "    Computes the weights and bias for a general n feature model.\n",
        "    :param chosen_features:  list of features to perform multiple regression on\n",
        "    :return: chosen_feature_matrix, computed weights and bias via regression\n",
        "    \"\"\"\n",
        "\n",
        "    # ToDo\n",
        "    #  Would selecting different initial weights make any difference?\n",
        "    #  Explain your answer.\n",
        "    keywords = {\n",
        "        'initial_weights': np.array([.5]*len(chosen_features)),\n",
        "        'step_size': 1.e-4,\n",
        "        'tolerance': 1.e-10,\n",
        "        'bias': 0\n",
        "    }\n",
        "\n",
        "    chosen_feature_dataframe = train_x[chosen_features]\n",
        "    # ToDo\n",
        "    #  Why are the features normalized?\n",
        "    chosen_feature_dataframe = normalize_features(chosen_features, chosen_feature_dataframe)\n",
        "    chosen_feature_matrix = chosen_feature_dataframe.to_numpy()\n",
        "\n",
        "    target_column = train_y\n",
        "    target_matrix = target_column.to_numpy()\n",
        "\n",
        "    train_weights, bias = n_feature_regression(chosen_feature_matrix, target_matrix, keywords)\n",
        "\n",
        "    return chosen_feature_matrix, train_weights, bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScZWZmSy89cP"
      },
      "source": [
        "## Two Feature Regression\n",
        "\n",
        "In this part, you should choose 2 features and implement multiple regression on them :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiVaUfi1Hgfy"
      },
      "outputs": [],
      "source": [
        "chosen_features = [] # ToDo\n",
        "# ToDo\n",
        "# compute the chosen_feature_matrix, train_weights, and bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20jP1GeHoT-y"
      },
      "outputs": [],
      "source": [
        "#ToDo\n",
        "# compute the predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afczu8-Ppyf_"
      },
      "outputs": [],
      "source": [
        "#ToDo\n",
        "# Calculate the R2 score and mean square error\n",
        "# Explain the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS7fNqOy88px"
      },
      "source": [
        "## Three Feature Regression\n",
        "\n",
        "Now repeat the steps for 3 features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHzyodmHuWhB"
      },
      "outputs": [],
      "source": [
        "# ToDo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDjkVx9RuV_I"
      },
      "source": [
        "## Five Feature Regression\n",
        "\n",
        "Finally, repeat the steps for 5 features\n",
        "\n",
        "Explain the differences in the results and the reasoning behind these variations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YKANjft7xQL"
      },
      "outputs": [],
      "source": [
        "# ToDo"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
